# EDA_and_Feature_Eng
EDA and Feature Engineering of data.here we use house pricing data to EDA and Feature engineering.
Exploratory Data Analysis is the process of visually and statistically summarizing, interpreting, and understanding the main characteristics of a dataset. EDA helps you get a sense of what your data looks like, identify potential relationships, patterns, and anomalies, and guide the subsequent steps in data preprocessing and modeling. Here are some key aspects of EDA:
# 1- EDA:
In Exploratory data anylysis we done following methods only to analyse the data.
 1.Data Summarization: You calculate basic summary statistics like mean, median, standard deviation, min, max, etc., to get an initial understanding of the distribution and scale of your data.
  
 2.Data Visualization: Creating various types of plots and visualizations, such as histograms, scatter plots, box plots, and 
   correlation matrices, to visually represent the distribution and relationships within the data.
   
 3.missing values present in data.
 
 4.all the numerical variables present- 1)discrete variable  2)continuous variable.
 
 5.saw the distribution of numerical variable and continuous variable variable.
 
 6.categorical feature and its distribution.
 
 7.temporal feature present in data.
 
 8.outliers present in each feature.
 
 9.reletion between dependent and independent feature.

 # 2-FEATURE ENGINEERING:
Feature engineering involves creating new features or modifying existing ones to enhance the performance of machine learning models. The goal is to extract relevant information from the raw data and present it in a way that helps the model understand patterns better. Effective feature engineering can significantly improve model accuracy and robustness. Here's what it involves:
1.Feature Creation: Designing and creating new features based on domain knowledge, mathematical transformations, 
 interactions between existing features, or other data transformations.

2.Feature Selection: Choosing the most relevant features that contribute the most to the predictive power of the model, 
 while removing irrelevant or redundant features that can add noise or slow down training.

3.Feature Scaling: Ensuring that features are on similar scales, which helps models that are sensitive to the scale of input 
 features, such as gradient-based algorithms.

4.Encoding Categorical Variables: Converting categorical variables into numerical representations that can be used by 
 machine learning algorithms, such as one-hot encoding or label encoding.

5.Binning and Bucketing: Grouping continuous variables into bins or buckets to capture non-linear relationships or reduce 
 noise.
 
6.handling missing values.

7.handling temporal variable.

8.handle categorical variable:remove rare labels.

